{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNK3aHv/EW9Z6GmxGUtaibN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/so-yeon-hwang/DataSciencePractice/blob/main/Data_Science_Mid_term.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk8R0Lp3wcvu"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문제 3"
      ],
      "metadata": {
        "id": "4udlA2YSzn3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.FloatTensor([[1],[1],[2],[3]])\n",
        "y_train = torch.FloatTensor([[1],[2],[3],[3]])\n",
        "\n",
        "W = torch.FloatTensor([[1]])\n",
        "b = torch.FloatTensor([[1]])\n",
        "\n",
        "lr = 0.1\n",
        "\n",
        "W,b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0TgLGS5xn0_",
        "outputId": "3f4675d9-d064-433e-b453-092ea2028dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1.]]), tensor([[1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W.requires_grad_(True)\n",
        "b.requires_grad_(True)\n",
        "\n",
        "h = torch.mm(x_train, W) + b\n",
        "cost = ((y_train - h) ** 2).mean()\n",
        "\n",
        "cost.backward() # Gradient calcualating function\n",
        "with torch.no_grad() as grd : # pause _ gradient calculating\n",
        "  W = W - lr * W.grad # updating W value : moving - direction\n",
        "  b = b - lr * b.grad\n",
        "\n",
        "print(cost.item(), W.squeeze(), b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSsRVxpyyUUw",
        "outputId": "3c5b02c4-623b-48ab-9804-6db922b0a34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5 tensor(0.9800) tensor([[0.9900]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 입력 데이터와 정답 데이터\n",
        "x_train = torch.FloatTensor([[1], [1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[1], [2], [3], [3]])\n",
        "\n",
        "# 초기 가중치 및 편향\n",
        "W = torch.FloatTensor([[1]])\n",
        "b = torch.FloatTensor([[1]])\n",
        "\n",
        "# 학습률\n",
        "lr = 0.01\n",
        "\n",
        "# 예측값 계산\n",
        "prediction = x_train * W + b\n",
        "\n",
        "# 오차 계산\n",
        "cost = ((y_train - h) ** 2).mean()\n",
        "\n",
        "# W와 b의 gradient 계산\n",
        "W_gradient = torch.mean(cost * x_train)\n",
        "b_gradient = torch.mean(cost)\n",
        "\n",
        "# Gradient Descent를 사용하여 W와 b 업데이트\n",
        "W = W - lr * W_gradient\n",
        "b = b - lr * b_gradient\n",
        "\n",
        "print(\"Updated W:\", W)\n",
        "print(\"Updated b:\", b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ54b9yFyEEd",
        "outputId": "228e6a7f-730c-45e1-cd66-8d6d340ff6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated W: tensor([[0.9912]], grad_fn=<SubBackward0>)\n",
            "Updated b: tensor([[0.9950]], grad_fn=<SubBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W_gradient = torch.mean(cost * x_train)\n",
        "W_gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNfP7dfwzRE2",
        "outputId": "e76905e7-74c5-4d53-b6b9-2ce9544d6a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8750, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문제 4"
      ],
      "metadata": {
        "id": "nsnN5LWTz9Bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_train = torch.FloatTensor([[1], [1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[1], [2], [3], [3]])\n",
        "\n",
        "w = torch.FloatTensor([[1]])\n",
        "b = torch.FloatTensor([[1]])\n",
        "\n",
        "for epoch in range(101):\n",
        "\n",
        "  w.requires_grad_(True)\n",
        "  b.requires_grad_(True)\n",
        "\n",
        "  h = torch.mm(x_train, w) + b\n",
        "  cost = ((y_train - h) ** 2).mean()"
      ],
      "metadata": {
        "id": "XgwElZQXzWjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.FloatTensor([[1]])\n",
        "w.requires_grad_(True)\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoGIgSCH2Hd_",
        "outputId": "a5879523-2109-4f38-c35a-01362d984e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_train,y_train)\n",
        "\n",
        "X = torch.FloatTensor([[0],[4]])\n",
        "Y = torch.mm(X,w) + b\n",
        "\n",
        "plt.plot(X,Y,c='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "G3v4eg0D1ciJ",
        "outputId": "70584e5f-54e4-4613-a719-cd8b0c2bc2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_train = torch.FloatTensor([[1], [1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[1], [2], [3], [3]])\n",
        "\n",
        "w = torch.FloatTensor([[1]])\n",
        "b = torch.FloatTensor([[1]])"
      ],
      "metadata": {
        "id": "cIvZGaf71nMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w,b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDJjNu392ltW",
        "outputId": "b941a7a7-0970-49f6-fc6d-ddb45bc5f25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1.]]), tensor([[1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(101):\n",
        "\n",
        "  w.requires_grad_(True)\n",
        "  b.requires_grad_(True)\n",
        "\n",
        "  h = torch.mm(x_train, w) + b\n",
        "  cost = ((y_train - h) ** 2).mean()\n",
        "\n",
        "  cost.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    w = w - 0.1*w.grad\n",
        "    b = b - 0.1*b.grad\n",
        "  print(epoch, cost.item(), W.squeeze(), b)"
      ],
      "metadata": {
        "id": "72IYWyl22ke8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.FloatTensor([[1], [1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[1], [2], [3], [3]])\n",
        "\n",
        "w = torch.FloatTensor([[1]])\n",
        "b = torch.FloatTensor([[1]])\n",
        "lr = 0.1\n",
        "\n",
        "w.requires_grad_(True)\n",
        "b.requires_grad_(True) # calculating Gradient 기울기를 계산한다  // requires grad True 로 설정\n",
        "\n",
        "# making hypothesis\n",
        "h = torch.mm(x_train, w) + b\n",
        "\n",
        "# gap btw true values vs hypothesis values\n",
        "cost = ((y_train - h) ** 2).mean()\n",
        "\n",
        "cost.backward() # Gradient calcualating function\n",
        "with torch.no_grad() as grd : # pause _ gradient calculating\n",
        "    w = w - lr * w.grad # updating W value : moving - direction\n",
        "    b = b - lr * b.grad\n",
        "\n",
        "print(cost.item(), W.squeeze(), b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl_T99aS3g-M",
        "outputId": "fa3f6e45-e6ff-4647-f192-4f60acbe9b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5 tensor(0.9800) tensor([[0.9000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.FloatTensor([[1], [1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[1], [2], [3], [3]])\n",
        "\n",
        "w = torch.FloatTensor([[1]])\n",
        "b = torch.FloatTensor([[1]])\n",
        "\n",
        "w.requires_grad_(True)\n",
        "b.requires_grad_(True)\n",
        "\n",
        "h = torch.mm(x_train, w) + b\n",
        "cost = ((y_train - h) ** 2).mean()\n",
        "\n",
        "cost.backward()\n",
        "\n",
        "with torch.no_grad():\n",
        "  w = w - 0.1*w.grad\n",
        "  b = b - 0.1*b.grad\n",
        "\n",
        "print(cost.item(), W.squeeze(), b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd5AWpZf1SqC",
        "outputId": "d1a1c345-6bdd-453d-f7d2-25b68f97f554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5 tensor(0.9800) tensor([[0.9000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yyCuUciT4F_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2페이지"
      ],
      "metadata": {
        "id": "cV159OU_4mhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "data = pickle.load(open('docs.pkl','rb'))\n",
        "\n",
        "# data['train'] 900개의 문서정보, 리스트 형태\n",
        "# data['test] 10개의 시험용 문서정보, 임베딩벡터\n",
        "\n",
        "# data[train][i] : i번 문서의 정보로, 128차원 벡터"
      ],
      "metadata": {
        "id": "7zzqjM0H4nBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# 12번 문서에서 가장 많이 등장한 단어 5개를 찾기\n",
        "doc_12 = data['train'][12]  # 12번 문서 (인덱스 11)"
      ],
      "metadata": {
        "id": "AbGiWKK_5GB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = doc_12['words']\n",
        "count"
      ],
      "metadata": {
        "id": "PaawtvWU5b53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 등장 횟수를 기준으로 내림차순으로 정렬\n",
        "sorted_count = sorted(count.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# 상위 5개 단어 추출\n",
        "top_words = sorted_count[:5]\n",
        "\n",
        "# 단어들을 띄어쓰기로 구분하여 출력\n",
        "top_words_str = ' '.join(word for word, count in top_words)\n",
        "\n",
        "print(top_words_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHmJQizD5dco",
        "outputId": "39c1d577-15d9-4f6c-80b0-f4adc5368955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the in to a conte\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'][31]"
      ],
      "metadata": {
        "id": "KOGMOAUE59bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서를 단어의 집합으로 생각할 때"
      ],
      "metadata": {
        "id": "m_WJTi916zTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(d1,d2):\n",
        "  if len(d1) == 0 or len(d2) == 0:\n",
        "    return 0\n",
        "  return len(d1&d2)/len(d1|d2)"
      ],
      "metadata": {
        "id": "Z5p8XLbajr0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jaccard_similarity()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69Cf_jacj92p",
        "outputId": "7970994b-a8bc-4e9e-8a58-8b11b409da02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.37142857142857144"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tf idf 는 서로 곱한 값"
      ],
      "metadata": {
        "id": "HQfXNW-97fLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = data['train'][1]['words'] # 딕셔너리\n",
        "\n",
        "\n",
        "# 단어 목록\n",
        "words = list(count.keys())\n",
        "\n",
        "# 각 단어의 등장 횟수를 값으로 갖는 벡터 생성\n",
        "word_vector = [count[word] for word in words]\n",
        "\n",
        "# 결과 출력\n",
        "print(word_vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8RG3OU07JkA",
        "outputId": "1e471a7e-30db-4c5f-980a-e59cb9097729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 1, 1, 1, 1, 1, 1, 1, 1, 5, 2, 6, 2, 1, 1, 3, 1, 1, 3, 1, 1, 5, 2, 1, 1, 1, 4, 1, 2, 1, 11, 1, 2, 1, 1, 1, 1, 2, 4, 2, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# genre : from json string to python set\n",
        "# make a new function\n",
        "\n",
        "def str_to_set(x):\n",
        "  word_set = set() # make an empty set\n",
        "  for item in eval(x): # eval function > python object\n",
        "    word_set.add(data['train']['words'])\n",
        "  return word_set\n"
      ],
      "metadata": {
        "id": "ipsCtp6P8oJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'] = data.train.words.apply(str_to_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "SczhpsJv9SMO",
        "outputId": "1d25b311-e9e8-4e47-90d3-ef067c9c1d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-529caf7c4fe1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_to_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'][31]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SBy-wZE9ZC8",
        "outputId": "bc4c867f-5b96-48c3-965c-d9b422151ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'Digital guru floats sub-$100 PC\\n\\nNicholas Negroponte, chairman and founder of MIT\\'s Media Labs, says he is developing a laptop PC that will go on sale for less than $100 (£53).\\n\\nHe told the BBC World Service programme Go Digital he hoped it would become an education tool in developing countries. He said one laptop per child could be \" very important to the development of not just that child but now the whole family, village and neighbourhood\". He said the child could use the laptop like a text book. He described the device as a stripped down laptop, which would run a Linux-based operating system, \"We have to get the display down to below $20, to do this we need to rear project the image rather than using an ordinary flat panel.\\n\\n\"The second trick is to get rid of the fat , if you can skinny it down you can gain speed and the ability to use smaller processors and slower memory.\" The device will probably be exported as a kit of parts to be assembled locally to keep costs down. Mr Negroponte said this was a not for profit venture, though he recognised that the manufacturers of the components would be making money. In 1995 Mr Negroponte published the bestselling Being Digital, now widely seen as predicting the digital age. The concept is based on experiments in the US state of Maine, where children were given laptop computers to take home and do their work on.\\n\\nWhile the idea was popular amongst the children, it initially received some resistance from the teachers and there were problems with laptops getting broken. However, Mr Negroponte has adapted the idea to his own work in Cambodia where he set up two schools together with his wife and gave the children laptops. \"We put in 25 laptops three years ago , only one has been broken, the kids cherish these things, it\\'s also a TV a telephone and a games machine, not just a textbook.\" Mr Negroponte wants the laptops to become more common than mobile phones but conceded this was ambitious. \"Nokia make 200 million cell phones a year, so for us to claim we\\'re going to make 200 million laptops is a big number, but we\\'re not talking about doing it in three or five years, we\\'re talking about months.\" He plans to be distributing them by the end of 2006 and is already in discussion with the Chinese education ministry who are expected to make a large order. \"In China they spend $17 per child per year on textbooks. That\\'s for five or six years, so if we can distribute and sell laptops in quantities of one million or more to ministries of education that\\'s cheaper and the marketing overheads go away.\"\\n',\n",
              " 'words': {'digital': 4,\n",
              "  'guru': 1,\n",
              "  'floats': 1,\n",
              "  'sub100': 1,\n",
              "  'pc': 2,\n",
              "  'nicholas': 1,\n",
              "  'negroponte': 5,\n",
              "  'chairman': 1,\n",
              "  'and': 11,\n",
              "  'founder': 1,\n",
              "  'of': 9,\n",
              "  'mits': 1,\n",
              "  'media': 1,\n",
              "  'labs': 1,\n",
              "  'says': 1,\n",
              "  'he': 9,\n",
              "  'is': 5,\n",
              "  'developing': 2,\n",
              "  'a': 13,\n",
              "  'laptop': 5,\n",
              "  'that': 3,\n",
              "  'will': 2,\n",
              "  'go': 3,\n",
              "  'on': 4,\n",
              "  'sale': 1,\n",
              "  'for': 4,\n",
              "  'less': 1,\n",
              "  'than': 3,\n",
              "  '100': 1,\n",
              "  '£53': 1,\n",
              "  'told': 1,\n",
              "  'the': 28,\n",
              "  'bbc': 1,\n",
              "  'world': 1,\n",
              "  'service': 1,\n",
              "  'programme': 1,\n",
              "  'hoped': 1,\n",
              "  'it': 4,\n",
              "  'would': 3,\n",
              "  'become': 2,\n",
              "  'an': 2,\n",
              "  'education': 3,\n",
              "  'tool': 1,\n",
              "  'in': 9,\n",
              "  'countries': 1,\n",
              "  'said': 3,\n",
              "  'one': 3,\n",
              "  'per': 3,\n",
              "  'child': 4,\n",
              "  'could': 2,\n",
              "  'be': 5,\n",
              "  'very': 1,\n",
              "  'important': 1,\n",
              "  'to': 17,\n",
              "  'development': 1,\n",
              "  'not': 4,\n",
              "  'just': 2,\n",
              "  'but': 3,\n",
              "  'now': 2,\n",
              "  'whole': 1,\n",
              "  'family': 1,\n",
              "  'village': 1,\n",
              "  'neighbourhood': 1,\n",
              "  'use': 2,\n",
              "  'like': 1,\n",
              "  'text': 1,\n",
              "  'book': 1,\n",
              "  'described': 1,\n",
              "  'device': 2,\n",
              "  'as': 3,\n",
              "  'stripped': 1,\n",
              "  'down': 4,\n",
              "  'which': 1,\n",
              "  'run': 1,\n",
              "  'linuxbased': 1,\n",
              "  'operating': 1,\n",
              "  'system': 1,\n",
              "  'we': 4,\n",
              "  'have': 1,\n",
              "  'get': 2,\n",
              "  'display': 1,\n",
              "  'below': 1,\n",
              "  '20': 1,\n",
              "  'do': 2,\n",
              "  'this': 3,\n",
              "  'need': 1,\n",
              "  'rear': 1,\n",
              "  'project': 1,\n",
              "  'image': 1,\n",
              "  'rather': 1,\n",
              "  'using': 1,\n",
              "  'ordinary': 1,\n",
              "  'flat': 1,\n",
              "  'panel': 1,\n",
              "  'second': 1,\n",
              "  'trick': 1,\n",
              "  'rid': 1,\n",
              "  'fat': 1,\n",
              "  'if': 2,\n",
              "  'you': 2,\n",
              "  'can': 3,\n",
              "  'skinny': 1,\n",
              "  'gain': 1,\n",
              "  'speed': 1,\n",
              "  'ability': 1,\n",
              "  'smaller': 1,\n",
              "  'processors': 1,\n",
              "  'slower': 1,\n",
              "  'memory': 1,\n",
              "  'probably': 1,\n",
              "  'exported': 1,\n",
              "  'kit': 1,\n",
              "  'parts': 1,\n",
              "  'assembled': 1,\n",
              "  'locally': 1,\n",
              "  'keep': 1,\n",
              "  'costs': 1,\n",
              "  'mr': 4,\n",
              "  'was': 3,\n",
              "  'profit': 1,\n",
              "  'venture': 1,\n",
              "  'though': 1,\n",
              "  'recognised': 1,\n",
              "  'manufacturers': 1,\n",
              "  'components': 1,\n",
              "  'making': 1,\n",
              "  'money': 1,\n",
              "  '1995': 1,\n",
              "  'published': 1,\n",
              "  'bestselling': 1,\n",
              "  'being': 1,\n",
              "  'widely': 1,\n",
              "  'seen': 1,\n",
              "  'predicting': 1,\n",
              "  'age': 1,\n",
              "  'concept': 1,\n",
              "  'based': 1,\n",
              "  'experiments': 1,\n",
              "  'us': 2,\n",
              "  'state': 1,\n",
              "  'maine': 1,\n",
              "  'where': 2,\n",
              "  'children': 3,\n",
              "  'were': 5,\n",
              "  'given': 1,\n",
              "  'computers': 1,\n",
              "  'take': 1,\n",
              "  'home': 1,\n",
              "  'their': 1,\n",
              "  'work': 2,\n",
              "  'while': 1,\n",
              "  'idea': 2,\n",
              "  'popular': 1,\n",
              "  'amongst': 1,\n",
              "  'initially': 1,\n",
              "  'received': 1,\n",
              "  'some': 1,\n",
              "  'resistance': 1,\n",
              "  'from': 1,\n",
              "  'teachers': 1,\n",
              "  'there': 1,\n",
              "  'problems': 1,\n",
              "  'with': 3,\n",
              "  'laptops': 6,\n",
              "  'getting': 1,\n",
              "  'broken': 2,\n",
              "  'however': 1,\n",
              "  'has': 2,\n",
              "  'adapted': 1,\n",
              "  'his': 2,\n",
              "  'own': 1,\n",
              "  'cambodia': 1,\n",
              "  'set': 1,\n",
              "  'up': 1,\n",
              "  'two': 1,\n",
              "  'schools': 1,\n",
              "  'together': 1,\n",
              "  'wife': 1,\n",
              "  'gave': 1,\n",
              "  'put': 1,\n",
              "  '25': 1,\n",
              "  'three': 2,\n",
              "  'years': 3,\n",
              "  'ago': 1,\n",
              "  'only': 1,\n",
              "  'been': 1,\n",
              "  'kids': 1,\n",
              "  'cherish': 1,\n",
              "  'these': 1,\n",
              "  'things': 1,\n",
              "  'its': 1,\n",
              "  'also': 1,\n",
              "  'tv': 1,\n",
              "  'telephone': 1,\n",
              "  'games': 1,\n",
              "  'machine': 1,\n",
              "  'textbook': 1,\n",
              "  'wants': 1,\n",
              "  'more': 2,\n",
              "  'common': 1,\n",
              "  'mobile': 1,\n",
              "  'phones': 2,\n",
              "  'conceded': 1,\n",
              "  'ambitious': 1,\n",
              "  'nokia': 1,\n",
              "  'make': 3,\n",
              "  '200': 2,\n",
              "  'million': 3,\n",
              "  'cell': 1,\n",
              "  'year': 2,\n",
              "  'so': 2,\n",
              "  'claim': 1,\n",
              "  'going': 1,\n",
              "  'big': 1,\n",
              "  'number': 1,\n",
              "  'talking': 2,\n",
              "  'about': 2,\n",
              "  'doing': 1,\n",
              "  'or': 3,\n",
              "  'five': 2,\n",
              "  'months': 1,\n",
              "  'plans': 1,\n",
              "  'distributing': 1,\n",
              "  'them': 1,\n",
              "  'by': 1,\n",
              "  'end': 1,\n",
              "  '2006': 1,\n",
              "  'already': 1,\n",
              "  'discussion': 1,\n",
              "  'chinese': 1,\n",
              "  'ministry': 1,\n",
              "  'who': 1,\n",
              "  'are': 1,\n",
              "  'expected': 1,\n",
              "  'large': 1,\n",
              "  'order': 1,\n",
              "  'china': 1,\n",
              "  'they': 1,\n",
              "  'spend': 1,\n",
              "  '17': 1,\n",
              "  'textbooks': 1,\n",
              "  'thats': 2,\n",
              "  'six': 1,\n",
              "  'distribute': 1,\n",
              "  'sell': 1,\n",
              "  'quantities': 1,\n",
              "  'ministries': 1,\n",
              "  'cheaper': 1,\n",
              "  'marketing': 1,\n",
              "  'overheads': 1,\n",
              "  'away': 1},\n",
              " 'category': 'technologie',\n",
              " 'embedding': [-0.009534184319486603,\n",
              "  -0.0022370686190877707,\n",
              "  -0.003754631674844324,\n",
              "  -0.0035601109408320434,\n",
              "  -0.000559675606159996,\n",
              "  -0.0002752978328606747,\n",
              "  -0.0015748345740790638,\n",
              "  -0.004209119331641115,\n",
              "  0.0007869562176420235,\n",
              "  0.0007458880676494372,\n",
              "  0.0012044765931699065,\n",
              "  0.0008994805976167562,\n",
              "  0.0026480653633349466,\n",
              "  -0.008820584815469537,\n",
              "  0.0037554580604829122,\n",
              "  0.002357882080615494,\n",
              "  0.001858460093067314,\n",
              "  0.0037456163609980125,\n",
              "  -0.0021057863043074296,\n",
              "  -0.002935261724785328,\n",
              "  -0.001923312152204785,\n",
              "  -0.0016466462337022456,\n",
              "  0.00016596012890489117,\n",
              "  -0.002055373477706008,\n",
              "  0.0014336169371440537,\n",
              "  0.006783869612031779,\n",
              "  -0.004778800621884424,\n",
              "  -0.005702230111106836,\n",
              "  -0.0005003196730077826,\n",
              "  -0.0015615514992580915,\n",
              "  -0.0035832562505996248,\n",
              "  0.002473851636089052,\n",
              "  0.0010827272270177064,\n",
              "  -0.0020907073110420847,\n",
              "  -0.0015142597438451005,\n",
              "  0.002042492296486049,\n",
              "  0.0005543614563214099,\n",
              "  -0.0022067350723048726,\n",
              "  0.0016790829764217992,\n",
              "  -0.0001558456406570645,\n",
              "  0.0040531826825399506,\n",
              "  0.005987319534123408,\n",
              "  0.0006650551200641752,\n",
              "  -0.00026769842703147645,\n",
              "  0.0013478583780923779,\n",
              "  -0.0040668751267600636,\n",
              "  -0.0016095197915943628,\n",
              "  -0.001880379556239063,\n",
              "  0.0007510760473282693,\n",
              "  -0.00023342464012200928,\n",
              "  -0.0017704630987764767,\n",
              "  0.0005384095883444266,\n",
              "  0.001860698488523789,\n",
              "  0.00010186402491074278,\n",
              "  -0.0013180185765218483,\n",
              "  -0.0003747712552123346,\n",
              "  -0.0012971818747829183,\n",
              "  -0.0019270730811367035,\n",
              "  -0.00349579795509621,\n",
              "  -0.00022932334405945508,\n",
              "  -0.0024236960319173944,\n",
              "  -0.000531898952014279,\n",
              "  -0.0014042718773891222,\n",
              "  0.0030054891804907466,\n",
              "  -0.001284606782992241,\n",
              "  0.0018884458692345217,\n",
              "  0.0008124412691029868,\n",
              "  0.001064549706865167,\n",
              "  -0.00330273029135041,\n",
              "  -0.0034439473758539215,\n",
              "  -0.0043587172737267,\n",
              "  0.0025050786792686526,\n",
              "  -0.0005452880087674117,\n",
              "  0.0009974935064575065,\n",
              "  0.0014469667455236372,\n",
              "  0.0020821270229860788,\n",
              "  -0.0015929945472183224,\n",
              "  -0.001323905813307511,\n",
              "  -0.002847804611418576,\n",
              "  -0.00387810515625306,\n",
              "  0.003773208909872019,\n",
              "  -0.0008878842149775748,\n",
              "  0.003873327552458853,\n",
              "  0.006257868185968692,\n",
              "  -0.0045843140074284874,\n",
              "  0.0006824261965088389,\n",
              "  -0.003931761926097681,\n",
              "  0.001929242701227959,\n",
              "  -0.001988362056715563,\n",
              "  0.0009242224466026455,\n",
              "  0.00037676309814540864,\n",
              "  -0.0032650550406041535,\n",
              "  0.004006283591657617,\n",
              "  -0.0025988881860733444,\n",
              "  0.0013388393818246547,\n",
              "  -0.0003201592214578808,\n",
              "  -0.0033564897939030803,\n",
              "  -0.004010048533775629,\n",
              "  -0.001613754339772107,\n",
              "  -0.005385427158717722,\n",
              "  -0.0004104111437465428,\n",
              "  0.00210665055854774,\n",
              "  -0.0027800597230014114,\n",
              "  -0.0008294402465473118,\n",
              "  -0.005762945413824795,\n",
              "  0.002877707969121618,\n",
              "  -0.0019418111438465058,\n",
              "  -0.0024502339060946616,\n",
              "  0.00047124067195966575,\n",
              "  0.001354153596615938,\n",
              "  0.0034013693173673473,\n",
              "  -0.005259462224589175,\n",
              "  0.0036436509025811422,\n",
              "  -0.004866956279498116,\n",
              "  -0.0031589734493188797,\n",
              "  0.008161556570779788,\n",
              "  -0.004402578308521131,\n",
              "  0.00477843847963205,\n",
              "  -0.004247393740976618,\n",
              "  -0.0033750990026214104,\n",
              "  -0.0008547326841326484,\n",
              "  -0.0019503116913674166,\n",
              "  0.00706610195636956,\n",
              "  -0.0014556532923089003,\n",
              "  -0.005400204048143197,\n",
              "  0.0002146122912504789,\n",
              "  -0.0013110213255477345,\n",
              "  -0.004984402582159629]}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Jaccard 유사도 계산 함수 정의\n",
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union\n",
        "\n",
        "# 31번 문서의 단어 집합 가져오기\n",
        "doc_31_words = set(data['train'][31]['words'].keys())\n",
        "\n",
        "# 모든 다른 문서와의 유사도 계산\n",
        "similarities = []\n",
        "\n",
        "for i, doc in enumerate(data['train']):\n",
        "    if i == 31:\n",
        "        continue  # 자기 자신과의 유사도는 계산할 필요가 없습니다.\n",
        "\n",
        "    other_doc_words = set(doc['words'].keys())\n",
        "    similarity = jaccard_similarity(doc_31_words, other_doc_words)\n",
        "    similarities.append((i, similarity))\n",
        "\n",
        "# 유사도를 기준으로 내림차순으로 정렬\n",
        "similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# 상위 5개 유사한 문서 출력\n",
        "top_similar_docs = similarities[:5]\n",
        "\n",
        "for doc_idx, similarity in top_similar_docs:\n",
        "    print(f\"문서 {doc_idx}: 유사도 {similarity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5vjjy5A9iT1",
        "outputId": "1cc5124e-dd38-4866-e4c9-34042f807c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문서 182: 유사도 0.9960\n",
            "문서 323: 유사도 0.2014\n",
            "문서 659: 유사도 0.1781\n",
            "문서 749: 유사도 0.1756\n",
            "문서 742: 유사도 0.1753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "182 323 659 749 742"
      ],
      "metadata": {
        "id": "MY8VpLi7Adwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "technologie technologie technologie technologie politics"
      ],
      "metadata": {
        "id": "viI_4bCrAhXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'][182]['category']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fAsyAp72-NGG",
        "outputId": "79ff94af-8a6d-4845-dfd6-d5b21b6538a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'technologie'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'][323]['category']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JMf_5VZt-PIo",
        "outputId": "507b2284-66e8-4e4d-c563-3e8f20a1a580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'technologie'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'][659]['category']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0iIOfW3y-SYb",
        "outputId": "ba5ec8f8-d24c-4499-b389-05fe15486ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'technologie'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'][749]['category']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "83ssk1ra-VgI",
        "outputId": "50792e91-c241-413a-a198-46d6dcf82f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'technologie'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'][742]['category']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "noNQRaJd-XA4",
        "outputId": "556a9c06-364a-4795-e683-09e8efc9e90c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'politics'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 단어의 문서 집합 생성\n",
        "word_to_docs = {}\n",
        "for i, doc in enumerate(data['train']):\n",
        "    words = set(doc['words'].keys())\n",
        "    for word in words:\n",
        "        if word in word_to_docs:\n",
        "            word_to_docs[word].add(i)\n",
        "        else:\n",
        "            word_to_docs[word] = {i}\n",
        "\n",
        "# \"game\"과 다른 단어 간의 Jaccard 유사도 계산\n",
        "word_similarities = []\n",
        "\n",
        "word1 = \"game\"\n",
        "for word2 in word_to_docs.keys():\n",
        "    if word2 != word1:\n",
        "        docs_word1 = word_to_docs[word1]\n",
        "        docs_word2 = word_to_docs[word2]\n",
        "        jaccard_similarity = len(docs_word1.intersection(docs_word2)) / len(docs_word1.union(docs_word2))\n",
        "        word_similarities.append((word2, jaccard_similarity))\n",
        "\n",
        "# 유사도를 기준으로 내림차순으로 정렬\n",
        "word_similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# 상위 5개 유사한 단어 출력\n",
        "top_similar_words = word_similarities[:5]\n",
        "\n",
        "result = \" \".join([word for word, similarity in top_similar_words])\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQo16lOq-YQL",
        "outputId": "4b434ac8-ce63-4319-94c4-bb3a82b6b8f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gaming nintendo sony video sonys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# 31번 문서 선택\n",
        "doc = data['train'][31]\n",
        "\n",
        "# TF 계산\n",
        "tf_scores = {}\n",
        "total_words = sum(doc['words'].values())\n",
        "for word in doc['words']:\n",
        "    tf_scores[word] = doc['words'][word] / total_words\n",
        "\n",
        "# IDF 계산\n",
        "idf_scores = {}\n",
        "total_docs = len(data['train'])\n",
        "for word in doc['words']:\n",
        "    word_count = sum(1 for doc in data['train'] if word in doc['words'])\n",
        "    idf_scores[word] = math.log(total_docs / word_count)\n",
        "\n",
        "# TF-IDF 계산\n",
        "tfidf_scores = {}\n",
        "for word in doc['words']:\n",
        "    tfidf_scores[word] = tf_scores[word] * idf_scores[word]\n",
        "\n",
        "# TF-IDF가 높은 순으로 정렬\n",
        "top_tfidf_words = sorted(tfidf_scores, key=tfidf_scores.get, reverse=True)[:5]\n",
        "\n",
        "result = \" \".join(top_tfidf_words)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndQBROC5-wk3",
        "outputId": "08c4c26c-d971-4d6e-9cb7-7171969d0de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "laptops negroponte laptop child digital\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# 31번 문서의 임베딩 벡터\n",
        "embedding_31 = np.array(data['train'][31]['embedding']).reshape(1, -1)\n",
        "\n",
        "# 모든 문서의 임베딩 벡터 추출\n",
        "all_embeddings = [np.array(doc['embedding']) for doc in data['train']]\n",
        "\n",
        "# Cosine Similarity 계산\n",
        "similarities = cosine_similarity(embedding_31, all_embeddings)\n",
        "\n",
        "# 유사도가 높은 순으로 정렬\n",
        "similarities = similarities[0]  # 2D 배열을 1D로 변환\n",
        "sorted_indices = np.argsort(similarities)[::-1]  # 내림차순 정렬\n",
        "\n",
        "# 가장 유사한 문서 5개 추출\n",
        "top_indices = sorted_indices[1:6]  # 자기 자신(31번 문서)은 제외\n",
        "\n",
        "# 문서 번호와 분류 추출\n",
        "similar_docs = [f\"{index} {data['train'][index]['category']}\" for index in top_indices]\n",
        "\n",
        "# 결과 출력\n",
        "result = \" \".join(similar_docs)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7A1U1FV_EvI",
        "outputId": "d7cbe243-f07e-4433-a6cb-5d0d67e20153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "182 technologie 749 technologie 72 technologie 66 technologie 302 technologie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "182 technologie 749 technologie 72 technologie 66 technologie 302 technologie\n",
        "\n",
        "182 749 72 66 302\n",
        "technologie technologie technologie technologie technologie"
      ],
      "metadata": {
        "id": "nxdOW14bAPKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# 31번 문서의 임베딩 벡터\n",
        "embedding_31 = np.array(data['train'][31]['embedding']).reshape(1, -1)\n",
        "\n",
        "# 모든 문서의 임베딩 벡터 추출\n",
        "all_embeddings = [np.array(doc['embedding']) for doc in data['train']]\n",
        "\n",
        "# Cosine Similarity 계산\n",
        "cosine_similarities = cosine_similarity(embedding_31, all_embeddings)[0]\n",
        "\n",
        "# Jaccard Similarity 계산 함수\n",
        "def jaccard_similarity(doc1, doc2):\n",
        "    set1 = set(doc1)\n",
        "    set2 = set(doc2)\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1) + len(set2) - intersection\n",
        "    return intersection / union\n",
        "\n",
        "# 31번 문서의 단어 집합\n",
        "words_31 = set(data['train'][31]['words'].keys())\n",
        "\n",
        "# 모든 문서의 Jaccard Similarity 계산\n",
        "jaccard_similarities = []\n",
        "for doc in data['train']:\n",
        "    words_doc = set(doc['words'].keys())\n",
        "    jaccard_sim = jaccard_similarity(words_31, words_doc)\n",
        "    jaccard_similarities.append(jaccard_sim)\n",
        "\n",
        "# 상관관계 계산\n",
        "correlation = np.corrcoef(jaccard_similarities, cosine_similarities)[0, 1]\n",
        "\n",
        "# 결과 출력\n",
        "print(correlation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwTuNm_K_dT2",
        "outputId": "3b7aea70-b31c-4142-dcb0-e96d98828eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5404000524320155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 학습 데이터 및 테스트 데이터 로드\n",
        "X_train = np.array([doc['embedding'] for doc in data['train']])\n",
        "y_train = [doc['category'] for doc in data['train']]\n",
        "X_test = np.array([doc['embedding'] for doc in data['test']])\n",
        "\n",
        "# 분류 레이블을 원-핫 인코딩\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "lb = LabelBinarizer()\n",
        "y_train_encoded = lb.fit_transform(y_train)\n",
        "\n",
        "# Softmax Regression 모델 정의\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(len(set(y_train)), activation='softmax', input_shape=(128,))\n",
        "])\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(X_train, y_train_encoded, epochs=10, batch_size=32)\n",
        "\n",
        "# 테스트 데이터 예측\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 예측 결과 해석\n",
        "predicted_categories = [list(set(y_train))[np.argmax(pred)] for pred in y_pred]\n",
        "\n",
        "# 결과 출력\n",
        "predicted_categories_str = ' '.join(predicted_categories)\n",
        "print(predicted_categories_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCDOqvg9_pgI",
        "outputId": "b34ccb1a-0e07-434e-a24f-4a8ea2cd0b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "29/29 [==============================] - 1s 3ms/step - loss: 2.2985 - accuracy: 0.1044\n",
            "Epoch 2/10\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2932 - accuracy: 0.2222\n",
            "Epoch 3/10\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2883 - accuracy: 0.3311\n",
            "Epoch 4/10\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2834 - accuracy: 0.4122\n",
            "Epoch 5/10\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2785 - accuracy: 0.4811\n",
            "Epoch 6/10\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.2736 - accuracy: 0.4889\n",
            "Epoch 7/10\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2687 - accuracy: 0.5100\n",
            "Epoch 8/10\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2640 - accuracy: 0.5189\n",
            "Epoch 9/10\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2592 - accuracy: 0.5589\n",
            "Epoch 10/10\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2544 - accuracy: 0.5767\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "food medical business politics politics entertainment politics space graphics space\n"
          ]
        }
      ]
    }
  ]
}